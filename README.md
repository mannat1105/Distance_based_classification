# Distance_based_classification
Below are the answers to the questions asked for report:
Ans1. 
1. Euclidean Distance
2. Manhattan Distance
3. Minkowski Distance
4. Cosine Similarity
5. Hamming Distance 
6. Mahalanobis Distance
7. Chebyshev Distance

Ans2.
 1. Manhattan Distance-Applications in knowing distance between houses in organized cities, length of wire connections in an integrated circuit, etc.
 2.  Chebyshev distance is used quite often in computer-aided manufacturing applications such as in drilling machines, laser cutters, etc.
 3.  It is often used in warehouse logistics to calculate time taken by an overhead crane to move an object.
 4. Cosine distance is often used in finding similarities between documents, recommender systems, computing cohesion within clusters in unsupervised learning, human pose matching based on joint coordinates, etc.


   Ans3.
1. Euclidean Distance – Measures straight-line distance between points.
2. Manhattan Distance – Measures distance by summing absolute differences (grid-like movement).
3. Minkowski Distance – Generalized form of Euclidean and Manhattan distances.
4. Cosine Similarity – Measures the angle between two vectors, used in text analysis.
5. Hamming Distance – Counts differences in binary strings, used in error detection.
6. Mahalanobis Distance – Considers correlations between features, useful in anomaly detection.
7. Chebyshev Distance-It is defined on a vector space where the distance between two vectors is the greatest of their differences along any coordinate dimension.
   
Ans4. 
1. Helps in tuning hyperparameters like k in KNN.
2. Reduces overfitting by validating on different subsets of data.
3. Provides a more reliable estimate of model accuracy.
   

   Ans5.
High 𝑘
k (large neighbors count) → High bias, low variance (simpler model, may underfit).
Low 𝑘
k (small neighbors count) → Low bias, high variance (more flexible, may overfit).
Optimal 𝑘
k balances bias and variance for better predictions.
